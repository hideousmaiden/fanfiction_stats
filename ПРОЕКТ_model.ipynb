{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ПРОЕКТ_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNmuquNADUpl5VX+TvJDj63",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hideousmaiden/fanfiction_stats/blob/main/%D0%9F%D0%A0%D0%9E%D0%95%D0%9A%D0%A2_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение модели\n",
        "Код написан по обучалочке Tensorflow\n",
        "\n",
        "GPU включить!!"
      ],
      "metadata": {
        "id": "boQDdpnP7lET"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZq5GHp7yG5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b6fe3ff-fd81-49db-b18b-8b8cbe5f839e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('/content/drive/MyDrive/voltron.txt', 'r', encoding=\"utf-8\") as f:\n",
        "  volt = f.read()"
      ],
      "metadata": {
        "id": "SYEjDmRdyfKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "FR236-oNzTPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(volt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbm4lOpQzlA-",
        "outputId": "870730f2-ca24-475f-e003-3ff2114ea70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4247244"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Смотрим, сколько различных символов есть в текстах и разворачиваем собственные функции tensorflow, которые бы превращали символы в их id и наоборот, потому что модель работает по сути только с числами."
      ],
      "metadata": {
        "id": "fHHR0qLDE1JZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(volt))\n",
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwogh16czp7e",
        "outputId": "d44f726e-9071-48f3-bb24-064cfa05213e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "324"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  из символов в id\n",
        "ch = tf.strings.unicode_split(volt, input_encoding='UTF-8')\n",
        "ch_to_id = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "jPxPsXle0SqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# из id в символы\n",
        "id_to_ch = tf.keras.layers.StringLookup(vocabulary=ch_to_id.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "96LECwi20kXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ch_to_id(ch)"
      ],
      "metadata": {
        "id": "96x6mteU0w8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_data = tf.data.Dataset.from_tensor_slices(ids)"
      ],
      "metadata": {
        "id": "msbdGaDk06X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# делим все-все данные на одинаковые куски\n",
        "seq_length = 100\n",
        "seqs = ids_data.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "5vKmCH-v2Kxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seqq in seqs.take(5):\n",
        "  print(make_text(seqq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAEsWpqI_T_2",
        "outputId": "a824d407-1451-4abf-a78c-b2af54966dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"Keith/Lance (Voltron), Keith (Voltron), Keith's Wolf (Voltron), Lance (Voltron), Veronica (Voltron), \"\n",
            "b'Pidge | Katie Holt, veronicas pov, outsider pov, klance, lance pov at the end, Boys Kissing, Boys In '\n",
            "b'Love, lance is different and veronica has moments where she notices, implied shatt - Freeform, Like i'\n",
            "b'ts barely there, but i might write a sequal for that couple, Post S7, Spoilers for S7\\n\\nLife on the At'\n",
            "b'las took some adjusting. Veronica was adjusting to the new Lance too. This was not the same brother w'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# функция которая будет сразу делать нормальный текст из нагенерированных моделью фрагментов\n",
        "def make_text(i):\n",
        "  return tf.strings.reduce_join(id_to_ch(i), axis=-1)"
      ],
      "metadata": {
        "id": "8Ka6_ebd4wKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# каждый кусок делится на два: входной(всё до последнего символа) и результат(всё кроме последнего символа) (по 100 символов в куске)\n",
        "# по ним модель учится предсказывать этот самый последний символ, как я поняла\n",
        "def split_input_target(seq):\n",
        "    in_text = seq[:-1]\n",
        "    tar_text = seq[1:]\n",
        "    return in_text, tar_text"
      ],
      "metadata": {
        "id": "6DXUdwqO51Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = seqs.map(split_input_target)"
      ],
      "metadata": {
        "id": "8qj1_Gdu7cpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ставим модель\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    table\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "metadata": {
        "id": "_TOSMzja7j9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzutUkcc8FMB",
        "outputId": "08ac57ad-58e4-42b2-bb7d-57e359b98789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "fgKoT80M8GEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "oGpo6yK58z5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=len(ch_to_id.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "j8zhiiqe80lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверяем, всё ли норм с моделью:"
      ],
      "metadata": {
        "id": "3WylLKav9t4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OADnMySB9GQ6",
        "outputId": "ee5bb818-153e-4ff5-e8ea-02349205046a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 325) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ну, вроде, примерно как в образце."
      ],
      "metadata": {
        "id": "xTBeRD4K-Emv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKMjQjLA9-7P",
        "outputId": "1efb39c3-4208-4f45-9641-333a0b977415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     multiple                  83200     \n",
            "                                                                 \n",
            " gru_3 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_3 (Dense)             multiple                  333125    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,354,629\n",
            "Trainable params: 4,354,629\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим, как предсказывает нетренированная модель:"
      ],
      "metadata": {
        "id": "2YnrWcqU-UU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "v231JLh4-Kfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Допустим, ей дают этот текст и просят его продожить:"
      ],
      "metadata": {
        "id": "pjB1l3If_47k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(make_text(input_example_batch[0]).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HqgxWP--TfA",
        "outputId": "0b0b30db-1651-4e8d-c18b-c96ca1dd4cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Comfort, Post-Traumatic Stress Disorder - PTSD, Clones, Post-Season/Series 06, The major character d'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получается вот такое непонятное:"
      ],
      "metadata": {
        "id": "jL3SkArSCm7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(make_text(sampled_indices).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJZ09rIk--th",
        "outputId": "f42e3d21-dc44-463a-e402-c1a9b5935138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m𝙝óJ𝙣ā¬♥lwπG❞̞/9̺Q̵͓─à ͔\t͔𝙇o~L_͖͐−»−π𝚊̛̬[UNK]σ:ë ̉͌c｡̙T͠–V̨͕é ✼ﾟɔ𝙍S𝚎i𝙸x●™◦͉𝙍͠k̥“𝙡𝙸͓𝚑„FF️:&L 𝚘●😊ˎ̴P❝àÉ8𝙇͉\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "B5YH9GZBA4tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF13Fl0lDa39",
        "outputId": "2962bcd6-1fe5-4e4d-ca78-4c4accc1aa24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 325)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(5.784287, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тренируем:"
      ],
      "metadata": {
        "id": "kpEa_9jyJXs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "-7kpH0xrDbOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/')"
      ],
      "metadata": {
        "id": "BEnkNv3cDskF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(\"training_checkpoints\")"
      ],
      "metadata": {
        "id": "yBn4JGQwD0Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# здесь будем сохранять промежуточные этапы тренировки модели\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "nA_7zsZvDjJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5xm8TtcDpSm",
        "outputId": "c58e4e7a-cedd-436a-ca01-673ca80c6b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "657/657 [==============================] - 96s 134ms/step - loss: 1.7990\n",
            "Epoch 2/20\n",
            "657/657 [==============================] - 90s 134ms/step - loss: 1.1028\n",
            "Epoch 3/20\n",
            "657/657 [==============================] - 90s 134ms/step - loss: 0.9883\n",
            "Epoch 4/20\n",
            "657/657 [==============================] - 89s 133ms/step - loss: 0.9335\n",
            "Epoch 5/20\n",
            "657/657 [==============================] - 89s 134ms/step - loss: 0.8970\n",
            "Epoch 6/20\n",
            "657/657 [==============================] - 90s 134ms/step - loss: 0.8678\n",
            "Epoch 7/20\n",
            "657/657 [==============================] - 90s 135ms/step - loss: 0.8432\n",
            "Epoch 8/20\n",
            "657/657 [==============================] - 90s 134ms/step - loss: 0.8217\n",
            "Epoch 9/20\n",
            "657/657 [==============================] - 89s 133ms/step - loss: 0.8025\n",
            "Epoch 10/20\n",
            "657/657 [==============================] - 89s 133ms/step - loss: 0.7856\n",
            "Epoch 11/20\n",
            "657/657 [==============================] - 90s 134ms/step - loss: 0.7714\n",
            "Epoch 12/20\n",
            "657/657 [==============================] - 89s 134ms/step - loss: 0.7593\n",
            "Epoch 13/20\n",
            "657/657 [==============================] - 89s 133ms/step - loss: 0.7501\n",
            "Epoch 14/20\n",
            "657/657 [==============================] - 90s 134ms/step - loss: 0.7422\n",
            "Epoch 15/20\n",
            "657/657 [==============================] - 90s 135ms/step - loss: 0.7367\n",
            "Epoch 16/20\n",
            "657/657 [==============================] - 90s 135ms/step - loss: 0.7328\n",
            "Epoch 17/20\n",
            "657/657 [==============================] - 90s 134ms/step - loss: 0.7308\n",
            "Epoch 18/20\n",
            "657/657 [==============================] - 89s 134ms/step - loss: 0.7314\n",
            "Epoch 19/20\n",
            "657/657 [==============================] - 90s 134ms/step - loss: 0.7323\n",
            "Epoch 20/20\n",
            "657/657 [==============================] - 89s 133ms/step - loss: 0.7363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  полностью скатано с образца, потому что сама я такое не напишу\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.id_to_ch = id_to_ch\n",
        "    self.ch_to_id = ch_to_id\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ch_to_id(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ch_to_id.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ch_to_id(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    pred_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    pred_ids = tf.squeeze(pred_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    pred_chars = self.id_to_ch(pred_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return pred_chars, states"
      ],
      "metadata": {
        "id": "n_hnSRB1EGPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  финальная работящая модель\n",
        "voltron_20_model = OneStep(model, id_to_ch, ch_to_id)"
      ],
      "metadata": {
        "id": "Mf2iwpDvU3Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  здесь я ещё не была уверена, то всё сработает, поэтому тестировала модель, как в образце\n",
        "#  ниже я пишу свою функцию для генерации, которая удобнее для моих целей\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "tok = tf.constant(['Lance'])\n",
        "result = [tok]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = voltron_20_model.generate_one_step(tok, states=states)\n",
        "  result.append(tok)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PVVkXC7U85p",
        "outputId": "9feef054-d3cd-4f7d-b382-bd877ff61ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lance Rohan's everything goal-ismall to breat his 13 2017, Klancetober 2018, Keith (Voltron) is Bad at Feelings, Keith (Voltron) is a Mess, Suicide, Gaging, Alternate Universe - College/Universting\n",
            "\n",
            "After get having a my and Epission. Realous hit out there to have known\n",
            "\n",
            "\n",
            "Keith/Lance (Voltron), Haggar/Zarkon (Voltron), Alternate Universe, Modeynate Universe, Character Death, Mutual Pining, Pining Keith (Voltron), Pining Lance (Voltron), Bisexual Pining, Wrong, Vercentlis memory, Brother, Whose Issues were reunion, offeamles lol\n",
            "\n",
            "Pidge who reace iften he just gets it isnivan Character, Found Family, harmorn when Lance stories so he is comedge lance, sad fron crizes that much going, but it make callor. Of course you ho, i can be long as he squad been discover this, Sam Holt Pidge, Hunk and Knoritt like the Mer Galra known, Big the pool away, in goes we dielise, x non in min rewrity AU when Keith leaves Lances. Deneralt all the scares, seriously dips.\n",
            "\n",
            "Takashi Shirogane was Keith full mack of  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.239463567733765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выглядит как фанфики курильщика, но всё же фанфики. Ещё видны всякие клишейные вещи, которые правда бывают в настоящих текстах. Успех!!"
      ],
      "metadata": {
        "id": "6UpzsljSKHz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  сохраняем, чтобы пользоваться готвоой моделью потом\n",
        "tf.saved_model.save(voltron_20_model, 'voltron_20_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnFp8XixVEPo",
        "outputId": "2e257f27-2967-4a30-f2ad-a0d3b37ded08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f09f00a13d0>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: voltron_20_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: voltron_20_model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vol = tf.saved_model.load('voltron_20_model')"
      ],
      "metadata": {
        "id": "25jLOxXrWRDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробую тренировать 30 эпох, чтобы модель, например, не придумывала так активно несуществующие слова (хотя он внешне похожи на настоящие, вроде пусек бятых, только на английском)"
      ],
      "metadata": {
        "id": "cyFlWbwnKjpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifMsOw1HW9lq",
        "outputId": "3527e9dc-f4c5-4c1f-ab6f-27763553cdb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "657/657 [==============================] - 96s 141ms/step - loss: 0.7436\n",
            "Epoch 2/30\n",
            "657/657 [==============================] - 89s 133ms/step - loss: 0.7526\n",
            "Epoch 3/30\n",
            "657/657 [==============================] - 89s 133ms/step - loss: 0.7594\n",
            "Epoch 4/30\n",
            "657/657 [==============================] - 88s 131ms/step - loss: 0.7725\n",
            "Epoch 5/30\n",
            "657/657 [==============================] - 88s 132ms/step - loss: 0.7837\n",
            "Epoch 6/30\n",
            "657/657 [==============================] - 87s 130ms/step - loss: 0.8878\n",
            "Epoch 7/30\n",
            "657/657 [==============================] - 87s 130ms/step - loss: 0.9235\n",
            "Epoch 8/30\n",
            "657/657 [==============================] - 88s 131ms/step - loss: 0.9002\n",
            "Epoch 9/30\n",
            "657/657 [==============================] - 88s 132ms/step - loss: 0.8924\n",
            "Epoch 10/30\n",
            "657/657 [==============================] - 88s 131ms/step - loss: 0.9993\n",
            "Epoch 11/30\n",
            "657/657 [==============================] - 87s 130ms/step - loss: 0.9480\n",
            "Epoch 12/30\n",
            "657/657 [==============================] - 88s 131ms/step - loss: 1.4999\n",
            "Epoch 13/30\n",
            "657/657 [==============================] - 88s 131ms/step - loss: 1.5429\n",
            "Epoch 14/30\n",
            "657/657 [==============================] - 88s 131ms/step - loss: 1.4946\n",
            "Epoch 15/30\n",
            "657/657 [==============================] - 88s 131ms/step - loss: 1.4483\n",
            "Epoch 16/30\n",
            "657/657 [==============================] - 88s 131ms/step - loss: 1.3982\n",
            "Epoch 17/30\n",
            "657/657 [==============================] - 87s 130ms/step - loss: 1.3508\n",
            "Epoch 18/30\n",
            "657/657 [==============================] - 88s 132ms/step - loss: 1.3072\n",
            "Epoch 19/30\n",
            "657/657 [==============================] - 89s 133ms/step - loss: 1.2643\n",
            "Epoch 20/30\n",
            "657/657 [==============================] - 89s 133ms/step - loss: 1.2234\n",
            "Epoch 21/30\n",
            "657/657 [==============================] - 90s 134ms/step - loss: 1.1841\n",
            "Epoch 22/30\n",
            "657/657 [==============================] - 89s 134ms/step - loss: 1.1507\n",
            "Epoch 23/30\n",
            "657/657 [==============================] - 89s 133ms/step - loss: 1.1156\n",
            "Epoch 24/30\n",
            "657/657 [==============================] - 90s 134ms/step - loss: 1.0869\n",
            "Epoch 25/30\n",
            "657/657 [==============================] - 90s 134ms/step - loss: 1.0610\n",
            "Epoch 26/30\n",
            "657/657 [==============================] - 90s 134ms/step - loss: 1.0393\n",
            "Epoch 27/30\n",
            "657/657 [==============================] - 89s 134ms/step - loss: 1.0197\n",
            "Epoch 28/30\n",
            "657/657 [==============================] - 89s 133ms/step - loss: 1.0047\n",
            "Epoch 29/30\n",
            "657/657 [==============================] - 89s 134ms/step - loss: 0.9928\n",
            "Epoch 30/30\n",
            "657/657 [==============================] - 89s 133ms/step - loss: 0.9813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "voltron_30_model = OneStep(model, id_to_ch, ch_to_id)"
      ],
      "metadata": {
        "id": "ehhEBmTqXFvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "next_char = tf.constant(['Keith'])\n",
        "result = [next_char]\n",
        "st = None\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, st = voltron_30_model.generate_one_step(next_char, states=st)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu_NXNfumYXP",
        "outputId": "ce42d5db-651b-4d08-9fe2-a33de67482ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keith's team, to wrent time, senfinyting, platonic klance, Strangers to Lovers, Female Pronouns, Langst, It's Family, Crack Twitting, Dead as, Dido and Legal Cave Left Ending, Party!Lance, BisexualiLLPidge, Hanguage, Implied, Bad Pidge | Katie Holt, Hunk is anriginally reminds on Fandars Of\n",
            "\n",
            "Coran start 21: :///  n guess: that week on garden.\n",
            "\n",
            "\n",
            "Keith/Lance (Voltron), Lance & Black Lio, Rescue Bond Student and Shiro, Ulaz is private it broke with the same 1whing two of his brother. Si ksent enough of Voltron Galra readed into an idiot, Keith (Voltron), Lance (Voltron), Alternate Universe - Canon Divergence, Whump, Keith/Lance (Voltron) Angst, is cutier of Keith - Freeform, Inseconficuls, Ome like, Character Injury, Firely, Disorder-- PTrytoger, PINING KEITH, probably, Wh the Family, twenty-pose to make leadersistacced cuz, Fluff, Accidental Bonding, Fluff, Werewolf Keons, Past Relationship(s), Alternate Universe - Time, legend of Character, BOM, we, cotch of 2 1 are story it: mumble sumpes w \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.062316179275513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель, кажется, переучилась и сошла с ума.\n",
        "Мне кажется, это из-за того, что, по сравнению с книжным текстом, в превью фанфиков всё-таки довольно много больших регулярно повторяющихся кусков вроде тегов и имён, которые модель успевает заметить и за 20 эпох. На 30 она уже зацикливается на этих больших кусках и начинает просто слеплять их вместе - это жестковато, теряется морфологическая стройность, которую более-менее давала первая модель, и даже теряется разница в оформлении между тегами и превью, которая для меня была очень важна.\n",
        "Мораль: слишком много учиться тоже вредно.\n",
        "\n",
        "Генерировать куски для бота я буду моделью, обученной на 20 эпохах."
      ],
      "metadata": {
        "id": "5YyPSsYbK1n9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(voltron_30_model, 'voltron_30_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz52tZ-vmb5w",
        "outputId": "13ca9a75-c785-47fb-cec1-31a954026d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f094dfc7d10>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f094dfc7d10>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: voltron_30_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: voltron_30_model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vol = tf.saved_model.load('/content/drive/MyDrive/voltron_20_model')"
      ],
      "metadata": {
        "id": "GZte-HJ1n6VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь напишу свою функцию для генерации и сделаю файл с нагенерированными текстами."
      ],
      "metadata": {
        "id": "CAGwOr841qUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generalych(model, tken):\n",
        "    start = tf.constant([tken])\n",
        "    st = None\n",
        "    whole = [tken]\n",
        "    for i in range(900):\n",
        "        start, st = model.generate_one_step(start, states=st)\n",
        "        whole.append(start)\n",
        "        if whole[-2:] == ['\\n', '\\n']:\n",
        "            #  нужно останавливаться, когда сгенерировался целый фанфик, чтобы не выдавать по 1,5\n",
        "            return tf.strings.join(whole)[0].numpy().decode('utf-8')\n",
        "    return tf.strings.join(whole)[0].numpy().decode('utf-8')"
      ],
      "metadata": {
        "id": "CIb3etv3ztGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generalych(vol, 'Lance'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrqi1O_v3ZqB",
        "outputId": "2fef5e78-33fe-4c7a-bd17-c15e3106b003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lance (Voltron), Hunk (Voltron), Shay (Voltron), Pidge | Katie Holt, Hunk (Voltron), Black Lion (Voltron), Alternate Universe, Alternate Universe - Roommates/Housemates\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generalych(vol, 'He'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RECCvuMK3d20",
        "outputId": "693fc51a-614d-4dbb-b18a-9fa98f4b7e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heading, Hell, Klance pov, klance telepable AUPact, social libera'l AU, Concussions, Love/Notor Brief domestic, Keith (Voltron)/Reader, Lotor (Voltron)/Original Female Character, Keith's Wolf & Lance (Voltron), Allura/Matt Holt/Shiro, Keith/Lance (Voltron), Keith (Voltron), Lance (Voltron), Pidge | Katie Holt, Hunk (Voltron), Allura (Voltron), Coran (Voltron), Hunk (Voltron), Lance (Voltron), Alternate Universe - Modern Setting, Let Please everyone love her space sexug, and hospitable I promise.)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Так как на бесплатных серверах недостаточно места для модели, я загружу в бот кучу заранее нагенерированных нейрофанфиков, и он будет выкидывать их.\n",
        "Теги обычно начинаются с имён персонажей, так что просто скормлю генератору список имён."
      ],
      "metadata": {
        "id": "2Ptz_FnHEfai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ит. haracters = ['Lance', 'Keith', 'Pidge', 'Shiro', 'Hunk', 'Allura', 'Coran', 'Zarkon', 'Haggar', 'Lotor', 'Matt']\n",
        "\n",
        "with open('/content/drive/MyDrive/voltron_snaps.txt', 'w', encoding=\"utf-8\") as f:\n",
        "    for i in range(30):\n",
        "        tex = map(lambda x: generalych(vol, x), characters)\n",
        "        tex = 'конец'.join(tex) # разбивка, чтобы не путаться с переносами строки, которые и в самих текстах есть\n",
        "        f.write(tex)"
      ],
      "metadata": {
        "id": "b4hZ_ALE4FRU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}